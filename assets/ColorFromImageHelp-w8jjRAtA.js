import{c as i,j as e,H as Ze,F as Je}from"./index-BDGaIHWC.js";const Qe=()=>{const s=i.c(1);let t;return s[0]===Symbol.for("react.memo_cache_sentinel")?(t=e.jsxs("svg",{width:"18",height:"18",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",strokeLinecap:"round",strokeLinejoin:"round",children:[e.jsx("rect",{x:"3",y:"3",width:"18",height:"18",rx:"2"}),e.jsx("circle",{cx:"8.5",cy:"8.5",r:"1.5"}),e.jsx("polyline",{points:"21 15 16 10 5 21"})]}),s[0]=t):t=s[0],t},Xe=()=>{const s=i.c(1);let t;return s[0]===Symbol.for("react.memo_cache_sentinel")?(t=e.jsxs("svg",{width:"18",height:"18",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",strokeLinecap:"round",strokeLinejoin:"round",children:[e.jsx("circle",{cx:"12",cy:"12",r:"3"}),e.jsx("path",{d:"M19.4 15a1.65 1.65 0 0 0 .33 1.82l.06.06a2 2 0 0 1-2.83 2.83l-.06-.06a1.65 1.65 0 0 0-1.82-.33 1.65 1.65 0 0 0-1 1.51V21a2 2 0 0 1-4 0v-.09A1.65 1.65 0 0 0 9 19.4a1.65 1.65 0 0 0-1.82.33l-.06.06a2 2 0 0 1-2.83-2.83l.06-.06A1.65 1.65 0 0 0 4.68 15a1.65 1.65 0 0 0-1.51-1H3a2 2 0 0 1 0-4h.09A1.65 1.65 0 0 0 4.6 9a1.65 1.65 0 0 0-.33-1.82l-.06-.06a2 2 0 0 1 2.83-2.83l.06.06A1.65 1.65 0 0 0 9 4.68a1.65 1.65 0 0 0 1-1.51V3a2 2 0 0 1 4 0v.09a1.65 1.65 0 0 0 1 1.51 1.65 1.65 0 0 0 1.82-.33l.06-.06a2 2 0 0 1 2.83 2.83l-.06.06A1.65 1.65 0 0 0 19.4 9a1.65 1.65 0 0 0 1.51 1H21a2 2 0 0 1 0 4h-.09a1.65 1.65 0 0 0-1.51 1z"})]}),s[0]=t):t=s[0],t},Ye=()=>{const s=i.c(1);let t;return s[0]===Symbol.for("react.memo_cache_sentinel")?(t=e.jsx("svg",{width:"18",height:"18",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",strokeLinecap:"round",strokeLinejoin:"round",children:e.jsx("polygon",{points:"12 2 15.09 8.26 22 9.27 17 14.14 18.18 21.02 12 17.77 5.82 21.02 7 14.14 2 9.27 8.91 8.26 12 2"})}),s[0]=t):t=s[0],t},es=()=>{const s=i.c(1);let t;return s[0]===Symbol.for("react.memo_cache_sentinel")?(t=e.jsxs("svg",{width:"18",height:"18",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",strokeLinecap:"round",strokeLinejoin:"round",children:[e.jsx("polyline",{points:"16 18 22 12 16 6"}),e.jsx("polyline",{points:"8 6 2 12 8 18"})]}),s[0]=t):t=s[0],t},ss=()=>{const s=i.c(1);let t;return s[0]===Symbol.for("react.memo_cache_sentinel")?(t=e.jsxs("svg",{width:"18",height:"18",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",strokeLinecap:"round",strokeLinejoin:"round",children:[e.jsx("path",{d:"M4 19.5A2.5 2.5 0 0 1 6.5 17H20"}),e.jsx("path",{d:"M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"})]}),s[0]=t):t=s[0],t};function is(){const s=i.c(100);let t;s[0]===Symbol.for("react.memo_cache_sentinel")?(t=e.jsx(Ze,{}),s[0]=t):t=s[0];let l,c,r;s[1]===Symbol.for("react.memo_cache_sentinel")?(l=e.jsx("div",{className:"cfih-hero-eyebrow",children:"Technical Reference"}),c=e.jsx("h1",{children:"ColorFromImage — Technical Writeup"}),r=e.jsx("p",{className:"cfih-hero-subtitle",children:"Extract a perceptually accurate color palette from any image, purely in the browser — with zero external dependencies."}),s[1]=l,s[2]=c,s[3]=r):(l=s[1],c=s[2],r=s[3]);let a;s[4]===Symbol.for("react.memo_cache_sentinel")?(a=e.jsxs("div",{className:"cfih-meta-badge",children:[e.jsx("span",{children:"Component"}),e.jsx("span",{children:"ColorFromImage.tsx"})]}),s[4]=a):a=s[4];let n;s[5]===Symbol.for("react.memo_cache_sentinel")?(n=e.jsxs("div",{className:"cfih-meta-badge",children:[e.jsx("span",{children:"Purpose"}),e.jsx("span",{children:"Extract dominant palette from an image"})]}),s[5]=n):n=s[5];let o;s[6]===Symbol.for("react.memo_cache_sentinel")?(o=e.jsxs("div",{className:"cfih-hero",children:[l,c,r,e.jsxs("div",{className:"cfih-meta-row",children:[a,n,e.jsxs("div",{className:"cfih-meta-badge",children:[e.jsx("span",{children:"Dependencies"}),e.jsx("span",{children:"Zero external libraries"})]})]})]}),s[6]=o):o=s[6];let h;s[7]===Symbol.for("react.memo_cache_sentinel")?(h=e.jsxs("h2",{className:"cfih-section-title",children:[e.jsx(Qe,{})," Overview"]}),s[7]=h):h=s[7];let d;s[8]===Symbol.for("react.memo_cache_sentinel")?(d=e.jsx("code",{children:"ColorFromImage"}),s[8]=d):d=s[8];let m;s[9]===Symbol.for("react.memo_cache_sentinel")?(m=e.jsxs("p",{children:[d," is a self-contained React component that extracts a dominant color palette from an image purely in the browser. It accepts images via drag-and-drop, clipboard paste (",e.jsx("code",{children:"Ctrl+V"}),"), a file prop, or a URL prop — and calls back with an ordered array of hex colors representing the image's palette."]}),s[9]=m):m=s[9];let p;s[10]===Symbol.for("react.memo_cache_sentinel")?(p=e.jsxs("section",{className:"cfih-section",children:[h,e.jsxs("div",{className:"cfih-prose",children:[m,e.jsxs("p",{children:["All color math is implemented from scratch using the ",e.jsx("strong",{children:"OKLab perceptual color space"}),", which means the palette reflects how humans actually see color, not just how pixels are numerically distributed."]})]})]}),s[10]=p):p=s[10];let x;s[11]===Symbol.for("react.memo_cache_sentinel")?(x=e.jsxs("h2",{className:"cfih-section-title",children:[e.jsx(Xe,{})," How It Works — Step by Step"]}),s[11]=x):x=s[11];let f;s[12]===Symbol.for("react.memo_cache_sentinel")?(f=e.jsxs("div",{className:"cfih-step-header",children:[e.jsx("span",{className:"cfih-step-num",children:"1"}),e.jsx("h3",{className:"cfih-step-title",children:"Image Ingestion"})]}),s[12]=f):f=s[12];let j;s[13]===Symbol.for("react.memo_cache_sentinel")?(j=e.jsx("p",{children:"The component supports four input paths, all funneling into the same processing pipeline:"}),s[13]=j):j=s[13];let b,u,y;s[14]===Symbol.for("react.memo_cache_sentinel")?(b=e.jsx("thead",{children:e.jsxs("tr",{children:[e.jsx("th",{children:"Source"}),e.jsx("th",{children:"Mechanism"})]})}),u=e.jsx("td",{children:"Drag and drop"}),y=e.jsx("code",{children:"onDrop"}),s[14]=b,s[15]=u,s[16]=y):(b=s[14],u=s[15],y=s[16]);let g,_,v;s[17]===Symbol.for("react.memo_cache_sentinel")?(g=e.jsxs("tr",{children:[u,e.jsxs("td",{children:[y," → reads file as ",e.jsx("code",{children:"Blob"})]})]}),_=e.jsx("td",{children:"Clipboard paste"}),v=e.jsx("code",{children:"window"}),s[17]=g,s[18]=_,s[19]=v):(g=s[17],_=s[18],v=s[19]);let N;s[20]===Symbol.for("react.memo_cache_sentinel")?(N=e.jsx("code",{children:"ClipboardItem"}),s[20]=N):N=s[20];let S,w,k;s[21]===Symbol.for("react.memo_cache_sentinel")?(S=e.jsxs("tr",{children:[_,e.jsxs("td",{children:[v," paste event → reads ",N," as ",e.jsx("code",{children:"Blob"})]})]}),w=e.jsxs("td",{children:[e.jsx("code",{children:"imageFile"})," prop"]}),k=e.jsx("code",{children:"File"}),s[21]=S,s[22]=w,s[23]=k):(S=s[21],w=s[22],k=s[23]);let C,L,I;s[24]===Symbol.for("react.memo_cache_sentinel")?(C=e.jsxs("tr",{children:[w,e.jsxs("td",{children:[k," object → reads as ",e.jsx("code",{children:"Blob"})]})]}),L=e.jsxs("td",{children:[e.jsx("code",{children:"imageUrl"})," prop"]}),I=e.jsx("code",{children:"Image"}),s[24]=C,s[25]=L,s[26]=I):(C=s[24],L=s[25],I=s[26]);let T;s[27]===Symbol.for("react.memo_cache_sentinel")?(T=e.jsx("div",{className:"cfih-table-wrap",children:e.jsxs("table",{className:"cfih-table",children:[b,e.jsxs("tbody",{children:[g,S,C,e.jsxs("tr",{children:[L,e.jsxs("td",{children:[I," with ",e.jsx("code",{children:'crossOrigin="anonymous"'})," → falls back to display-only if CORS blocks pixel access"]})]})]})]})}),s[27]=T):T=s[27];let A;s[28]===Symbol.for("react.memo_cache_sentinel")?(A=e.jsx("code",{children:"Blob"}),s[28]=A):A=s[28];let B;s[29]===Symbol.for("react.memo_cache_sentinel")?(B=e.jsx("code",{children:"FileReader"}),s[29]=B):B=s[29];let R;s[30]===Symbol.for("react.memo_cache_sentinel")?(R=e.jsx("code",{children:"HTMLImageElement"}),s[30]=R):R=s[30];let O;s[31]===Symbol.for("react.memo_cache_sentinel")?(O=e.jsxs("div",{className:"cfih-step",children:[f,e.jsxs("div",{className:"cfih-step-body",children:[j,T,e.jsxs("p",{children:["For ",A,"-based inputs, a ",B," converts the file to a data URL, which is then loaded into an ",R,". This guarantees same-origin access to pixel data — bypassing the CORS restriction that blocks ",e.jsx("code",{children:"getImageData()"})," on cross-origin images loaded directly."]})]})]}),s[31]=O):O=s[31];let E;s[32]===Symbol.for("react.memo_cache_sentinel")?(E=e.jsxs("div",{className:"cfih-step-header",children:[e.jsx("span",{className:"cfih-step-num",children:"2"}),e.jsx("h3",{className:"cfih-step-title",children:"Pixel Data Extraction"})]}),s[32]=E):E=s[32];let K;s[33]===Symbol.for("react.memo_cache_sentinel")?(K=e.jsx("code",{children:"<canvas>"}),s[33]=K):K=s[33];let F;s[34]===Symbol.for("react.memo_cache_sentinel")?(F=e.jsx("code",{children:"ctx.getImageData()"}),s[34]=F):F=s[34];let H,M;s[35]===Symbol.for("react.memo_cache_sentinel")?(H=e.jsxs("p",{children:["Once the image is loaded, it is drawn onto an off-screen ",K," ","at its native resolution. ",F," returns a flat"," ",e.jsx("code",{children:"Uint8ClampedArray"})," of RGBA values — four bytes per pixel."]}),M=e.jsx("pre",{className:"cfih-code-block",children:"[R, G, B, A, R, G, B, A, ...]"}),s[35]=H,s[36]=M):(H=s[35],M=s[36]);let G;s[37]===Symbol.for("react.memo_cache_sentinel")?(G=e.jsxs("div",{className:"cfih-step",children:[E,e.jsxs("div",{className:"cfih-step-body",children:[H,M,e.jsxs("p",{children:["Transparent pixels (",e.jsx("code",{children:"A < 128"}),") are skipped entirely, so PNG transparency is handled correctly."]})]})]}),s[37]=G):G=s[37];let z;s[38]===Symbol.for("react.memo_cache_sentinel")?(z=e.jsxs("div",{className:"cfih-step-header",children:[e.jsx("span",{className:"cfih-step-num",children:"3"}),e.jsx("h3",{className:"cfih-step-title",children:"Color Space Conversion — sRGB → OKLab"})]}),s[38]=z):z=s[38];let W;s[39]===Symbol.for("react.memo_cache_sentinel")?(W=e.jsxs("div",{className:"cfih-step",children:[z,e.jsxs("div",{className:"cfih-step-body",children:[e.jsxs("p",{children:["Every sampled pixel is converted from sRGB to ",e.jsx("strong",{children:"OKLab"})," before any clustering is done."]}),e.jsx("p",{children:"OKLab is a perceptually uniform color space designed by Björn Ottosson (2020). In OKLab:"}),e.jsxs("ul",{children:[e.jsxs("li",{children:[e.jsx("strong",{children:"Equal Euclidean distance = equal perceived color difference."})," This is not true in RGB, where a step of 10 in blue looks very different from a step of 10 in green."]}),e.jsxs("li",{children:[e.jsx("strong",{children:"Hue is stable"})," across lightness and chroma changes — a quality absent from both HSL and standard CIELAB."]})]}),e.jsx("p",{children:"The conversion pipeline is:"}),e.jsx("pre",{className:"cfih-code-block",children:"sRGB → linearize (gamma removal) → cone-response matrix → cube-root → OKLab matrix"}),e.jsx("p",{children:"All coefficients are embedded directly from the OKLab specification. No external library is needed."})]})]}),s[39]=W):W=s[39];let q;s[40]===Symbol.for("react.memo_cache_sentinel")?(q=e.jsxs("div",{className:"cfih-step-header",children:[e.jsx("span",{className:"cfih-step-num",children:"4"}),e.jsx("h3",{className:"cfih-step-title",children:"Saturation-Weighted Sampling"})]}),s[40]=q):q=s[40];let D;s[41]===Symbol.for("react.memo_cache_sentinel")?(D=e.jsxs("p",{children:["Rather than feeding every pixel into the clustering algorithm, the component samples pixels with a ",e.jsx("strong",{children:"chroma-based inclusion probability"}),":"]}),s[41]=D):D=s[41];let P,U;s[42]===Symbol.for("react.memo_cache_sentinel")?(P=e.jsx("div",{className:"cfih-table-wrap",children:e.jsxs("table",{className:"cfih-table",children:[e.jsx("thead",{children:e.jsxs("tr",{children:[e.jsx("th",{children:"OKLab Chroma"}),e.jsx("th",{children:"Inclusion Rate"})]})}),e.jsxs("tbody",{children:[e.jsxs("tr",{children:[e.jsx("td",{children:"< 0.05 (near-grey)"}),e.jsx("td",{children:"20%"})]}),e.jsxs("tr",{children:[e.jsx("td",{children:"0.05 – 0.15 (moderate)"}),e.jsx("td",{children:"60%"})]}),e.jsxs("tr",{children:[e.jsx("td",{children:"≥ 0.15 (vivid)"}),e.jsx("td",{children:"100%"})]})]})]})}),U=e.jsx("p",{children:"This is the key fix that separates this implementation from naive k-means on images."}),s[42]=P,s[43]=U):(P=s[42],U=s[43]);let V;s[44]===Symbol.for("react.memo_cache_sentinel")?(V=e.jsxs("div",{className:"cfih-callout",children:[e.jsx("strong",{children:"Why it matters:"})," In a photograph of a bird against a green forest, the background might account for 90% of all pixels. A uniform sampler would fill the palette almost entirely with shades of green. By up-weighting vivid pixels, small but visually dominant regions — the bird's bright plumage, a saturated flower, a neon sign — get proportional representation and form their own cluster instead of being absorbed."]}),s[44]=V):V=s[44];let $;s[45]===Symbol.for("react.memo_cache_sentinel")?($=e.jsxs("div",{className:"cfih-step",children:[q,e.jsxs("div",{className:"cfih-step-body",children:[D,P,U,V,e.jsxs("p",{children:["A stride step (",e.jsx("code",{children:"step = max(1, floor(total_pixels / 4000))"}),") limits the working set to ~4,000 pixels for performance, applied before the chroma filter."]})]})]}),s[45]=$):$=s[45];let Z;s[46]===Symbol.for("react.memo_cache_sentinel")?(Z=e.jsxs("div",{className:"cfih-step-header",children:[e.jsx("span",{className:"cfih-step-num",children:"5"}),e.jsx("h3",{className:"cfih-step-title",children:"K-Means++ Clustering in OKLab Space"})]}),s[46]=Z):Z=s[46];let J;s[47]===Symbol.for("react.memo_cache_sentinel")?(J=e.jsx("strong",{children:"k = 16"}),s[47]=J):J=s[47];let Q,X,Y,ee,se,te,ie;s[48]===Symbol.for("react.memo_cache_sentinel")?(Q=e.jsxs("p",{children:["The filtered pixel set is clustered into ",J," groups using"," ",e.jsx("strong",{children:"K-Means++"})," initialization."]}),X=e.jsx("p",{className:"cfih-subhead",children:"Why K-Means++?"}),Y=e.jsx("p",{children:"Standard k-means places initial centroids randomly, which can cause multiple centroids to start near the same cluster — leaving other parts of the color space uncovered. K-Means++ (Arthur & Vassilvitskii, 2007) instead seeds each new centroid with probability proportional to its squared distance from the nearest existing centroid, spreading centroids across the color space from the start and dramatically reducing bad initializations."}),ee=e.jsx("p",{className:"cfih-subhead",children:"Why k = 16?"}),se=e.jsx("p",{children:"A larger k means more clusters compete for palette slots. With k = 16, a dominant neutral-tone background cannot consume all available palette entries — there are enough slots left for accent colors. The final output can be trimmed to any target size by the consuming component."}),te=e.jsx("p",{className:"cfih-subhead",children:"The Clustering Loop"}),ie=e.jsx("pre",{className:"cfih-code-block",children:`For each iteration (max 20):
  1. Assign each pixel to its nearest centroid (OKLab Euclidean distance)
  2. Recompute each centroid as the mean of its assigned pixels
  3. Stop early if no assignments changed`}),s[48]=Q,s[49]=X,s[50]=Y,s[51]=ee,s[52]=se,s[53]=te,s[54]=ie):(Q=s[48],X=s[49],Y=s[50],ee=s[51],se=s[52],te=s[53],ie=s[54]);let le;s[55]===Symbol.for("react.memo_cache_sentinel")?(le=e.jsxs("div",{className:"cfih-step",children:[Z,e.jsxs("div",{className:"cfih-step-body",children:[Q,X,Y,ee,se,te,ie,e.jsxs("p",{children:["All distance math uses squared Euclidean distance in OKLab — squaring avoids a"," ",e.jsx("code",{children:"sqrt()"})," call per comparison, which is valid since relative ordering is all that matters for nearest-neighbor assignment."]})]})]}),s[55]=le):le=s[55];let ce;s[56]===Symbol.for("react.memo_cache_sentinel")?(ce=e.jsxs("div",{className:"cfih-step-header",children:[e.jsx("span",{className:"cfih-step-num",children:"6"}),e.jsx("h3",{className:"cfih-step-title",children:"Chroma-Biased Output Sorting"})]}),s[56]=ce):ce=s[56];let re,ae;s[57]===Symbol.for("react.memo_cache_sentinel")?(re=e.jsxs("p",{children:["After clustering converges, results are sorted by a"," ",e.jsx("strong",{children:"chroma-weighted score"})," rather than raw pixel count:"]}),ae=e.jsx("pre",{className:"cfih-code-block",children:"score = count × (1 + 3 × chroma)"}),s[57]=re,s[58]=ae):(re=s[57],ae=s[58]);let ne;s[59]===Symbol.for("react.memo_cache_sentinel")?(ne=e.jsx("code",{children:"500 × (1 + 0.60) = 800"}),s[59]=ne):ne=s[59];let oe;s[60]===Symbol.for("react.memo_cache_sentinel")?(oe=e.jsxs("div",{className:"cfih-step",children:[ce,e.jsxs("div",{className:"cfih-step-body",children:[re,ae,e.jsxs("p",{children:["A cluster of 500 vivid pixels (chroma = 0.20) scores"," ",ne," — higher than a neutral cluster of 700 pixels (chroma = 0.03) which scores ",e.jsx("code",{children:"700 × (1 + 0.09) = 763"}),"."]}),e.jsx("p",{children:"This ensures the returned palette leads with the colors that define the image's visual character, not just the colors that cover the most area."})]})]}),s[60]=oe):oe=s[60];let he;s[61]===Symbol.for("react.memo_cache_sentinel")?(he=e.jsxs("div",{className:"cfih-step-header",children:[e.jsx("span",{className:"cfih-step-num",children:"7"}),e.jsx("h3",{className:"cfih-step-title",children:"Canvas Rendering and State Management"})]}),s[61]=he):he=s[61];let de;s[62]===Symbol.for("react.memo_cache_sentinel")?(de=e.jsx("code",{children:"<canvas>"}),s[62]=de):de=s[62];let me;s[63]===Symbol.for("react.memo_cache_sentinel")?(me=e.jsx("code",{children:"width × height"}),s[63]=me):me=s[63];let pe;s[64]===Symbol.for("react.memo_cache_sentinel")?(pe=e.jsx("code",{children:"useLayoutEffect"}),s[64]=pe):pe=s[64];let xe;s[65]===Symbol.for("react.memo_cache_sentinel")?(xe=e.jsx("code",{children:"drawToCanvas"}),s[65]=xe):xe=s[65];let fe;s[66]===Symbol.for("react.memo_cache_sentinel")?(fe=e.jsx("code",{children:"onColorsExtracted"}),s[66]=fe):fe=s[66];let je;s[67]===Symbol.for("react.memo_cache_sentinel")?(je=e.jsxs("section",{className:"cfih-section",children:[x,O,G,W,$,le,oe,e.jsxs("div",{className:"cfih-step",children:[he,e.jsx("div",{className:"cfih-step-body",children:e.jsxs("p",{children:["Separately from the extraction pipeline, the component maintains a visible"," ",de," that mirrors the loaded image, letterboxed to fit the configured ",me,". This is handled via"," ",pe," refs — keeping ",xe," and"," ",fe," stable across renders without requiring them as"," ",e.jsx("code",{children:"useEffect"})," dependencies, which avoids stale-closure bugs."]})})]})]}),s[67]=je):je=s[67];let be;s[68]===Symbol.for("react.memo_cache_sentinel")?(be=e.jsxs("h2",{className:"cfih-section-title",children:[e.jsx(Ye,{})," What Makes It Better Than Naive Approaches"]}),s[68]=be):be=s[68];let ue;s[69]===Symbol.for("react.memo_cache_sentinel")?(ue=e.jsx("p",{className:"cfih-subhead",style:{marginTop:0},children:"vs. Simple RGB k-means"}),s[69]=ue):ue=s[69];let ye;s[70]===Symbol.for("react.memo_cache_sentinel")?(ye=e.jsx("div",{className:"cfih-table-wrap",children:e.jsxs("table",{className:"cfih-table",children:[e.jsx("thead",{children:e.jsxs("tr",{children:[e.jsx("th",{}),e.jsx("th",{children:"Naive RGB k-means"}),e.jsx("th",{children:"This implementation"})]})}),e.jsxs("tbody",{children:[e.jsxs("tr",{children:[e.jsx("td",{children:e.jsx("strong",{children:"Color space"})}),e.jsx("td",{children:"RGB (perceptually non-uniform)"}),e.jsx("td",{className:"cfih-table-check",children:"OKLab (perceptually uniform)"})]}),e.jsxs("tr",{children:[e.jsx("td",{children:e.jsx("strong",{children:"Sampling"})}),e.jsx("td",{children:"Uniform"}),e.jsx("td",{className:"cfih-table-check",children:"Chroma-weighted"})]}),e.jsxs("tr",{children:[e.jsx("td",{children:e.jsx("strong",{children:"Initialization"})}),e.jsx("td",{children:"Random"}),e.jsx("td",{className:"cfih-table-check",children:"K-Means++"})]}),e.jsxs("tr",{children:[e.jsx("td",{children:e.jsx("strong",{children:"k value"})}),e.jsx("td",{children:"Typically 8"}),e.jsx("td",{className:"cfih-table-check",children:"16 (more clusters, fewer collisions)"})]}),e.jsxs("tr",{children:[e.jsx("td",{children:e.jsx("strong",{children:"Output sort"})}),e.jsx("td",{children:"By pixel count"}),e.jsx("td",{className:"cfih-table-check",children:"By chroma-biased score"})]}),e.jsxs("tr",{children:[e.jsx("td",{children:e.jsx("strong",{children:"Vivid accent colors"})}),e.jsx("td",{children:"Frequently lost"}),e.jsx("td",{className:"cfih-table-check",children:"Preserved by up-weighting"})]})]})]})}),s[70]=ye):ye=s[70];let ge;s[71]===Symbol.for("react.memo_cache_sentinel")?(ge=e.jsxs("div",{className:"cfih-compare-card",children:[e.jsx("p",{className:"cfih-compare-card-title",children:"vs. HSL Histogram Methods"}),e.jsx("p",{children:"Methods like Google's dominant-color API find the peak of a hue histogram. They return at most one color and are blind to secondary hues. This implementation returns a full ranked palette — typically 10–16 usable colors covering the image's full chromatic range."})]}),s[71]=ge):ge=s[71];let _e;s[72]===Symbol.for("react.memo_cache_sentinel")?(_e=e.jsxs("div",{className:"cfih-compare-card",children:[e.jsx("p",{className:"cfih-compare-card-title",children:"vs. Median Cut (Heckbert 1979)"}),e.jsx("p",{children:"Median Cut recursively bisects bounding boxes. It is fast and deterministic, but ignores color distribution within each box — a bucket with one vivid pixel and 999 grey pixels will split the grey range, discarding the vivid color entirely. This component's chroma-weighted sampling directly solves that problem."})]}),s[72]=_e):_e=s[72];let ve;s[73]===Symbol.for("react.memo_cache_sentinel")?(ve=e.jsx("p",{className:"cfih-compare-card-title",children:"CORS Handling"}),s[73]=ve):ve=s[73];let Ne;s[74]===Symbol.for("react.memo_cache_sentinel")?(Ne=e.jsxs("section",{className:"cfih-section",children:[be,ue,ye,e.jsxs("div",{className:"cfih-compare-grid",children:[ge,_e,e.jsxs("div",{className:"cfih-compare-card",children:[ve,e.jsxs("p",{children:["The two-stage URL loading (attempt ",e.jsx("code",{children:'crossOrigin="anonymous"'})," first, fall back to display-only with error message) means the component never silently fails. The user is told exactly why extraction failed and what to do about it (upload the file directly), rather than returning an empty palette with no explanation."]})]})]})]}),s[74]=Ne):Ne=s[74];let Se,we;s[75]===Symbol.for("react.memo_cache_sentinel")?(Se=e.jsxs("h2",{className:"cfih-section-title",children:[e.jsx(es,{})," Component API"]}),we=e.jsx("pre",{className:"cfih-code-block",children:`<ColorFromImage
  width={400}                          // Canvas display width (px)
  height={300}                         // Canvas display height (px)
  borderColor="#cccccc"                // Drop zone border color
  borderThickness={1}                  // Border width (px)
  onColorsExtracted={(entry) => {}}    // Callback: { palette: string[] }
  imageUrl="https://..."               // Optional URL to load on mount
  imageFile={file}                     // Optional File object to load
  clearTrigger={n}                     // Increment to clear the canvas
/>`}),s[75]=Se,s[76]=we):(Se=s[75],we=s[76]);let ke;s[77]===Symbol.for("react.memo_cache_sentinel")?(ke={marginTop:16},s[77]=ke):ke=s[77];let Ce,Le,Ie,Te,Ae,Be,Re;s[78]===Symbol.for("react.memo_cache_sentinel")?(Ce=e.jsx("thead",{children:e.jsxs("tr",{children:[e.jsx("th",{children:"Prop"}),e.jsx("th",{children:"Type"}),e.jsx("th",{children:"Description"})]})}),Le=e.jsxs("tr",{children:[e.jsx("td",{children:e.jsx("span",{className:"cfih-api-prop",children:"width"})}),e.jsx("td",{children:e.jsx("span",{className:"cfih-api-type",children:"number"})}),e.jsx("td",{children:"Canvas display width in pixels"})]}),Ie=e.jsxs("tr",{children:[e.jsx("td",{children:e.jsx("span",{className:"cfih-api-prop",children:"height"})}),e.jsx("td",{children:e.jsx("span",{className:"cfih-api-type",children:"number"})}),e.jsx("td",{children:"Canvas display height in pixels"})]}),Te=e.jsxs("tr",{children:[e.jsx("td",{children:e.jsx("span",{className:"cfih-api-prop",children:"borderColor"})}),e.jsx("td",{children:e.jsx("span",{className:"cfih-api-type",children:"string"})}),e.jsx("td",{children:"Drop zone border color (CSS color value)"})]}),Ae=e.jsxs("tr",{children:[e.jsx("td",{children:e.jsx("span",{className:"cfih-api-prop",children:"borderThickness"})}),e.jsx("td",{children:e.jsx("span",{className:"cfih-api-type",children:"number"})}),e.jsx("td",{children:"Border width in pixels"})]}),Be=e.jsx("td",{children:e.jsx("span",{className:"cfih-api-prop",children:"onColorsExtracted"})}),Re=e.jsx("td",{children:e.jsxs("span",{className:"cfih-api-type",children:["(entry: ","{ palette: string[] }",") => void"]})}),s[78]=Ce,s[79]=Le,s[80]=Ie,s[81]=Te,s[82]=Ae,s[83]=Be,s[84]=Re):(Ce=s[78],Le=s[79],Ie=s[80],Te=s[81],Ae=s[82],Be=s[83],Re=s[84]);let Oe,Ee,Ke,Fe;s[85]===Symbol.for("react.memo_cache_sentinel")?(Oe=e.jsxs("tr",{children:[Be,Re,e.jsxs("td",{children:["Callback fired after extraction. ",e.jsx("code",{children:"palette[0]"})," is the most visually dominant color."]})]}),Ee=e.jsxs("tr",{children:[e.jsx("td",{children:e.jsx("span",{className:"cfih-api-prop",children:"imageUrl"})}),e.jsx("td",{children:e.jsx("span",{className:"cfih-api-type",children:"string"})}),e.jsx("td",{children:"Optional URL to load and analyze on mount"})]}),Ke=e.jsx("td",{children:e.jsx("span",{className:"cfih-api-prop",children:"imageFile"})}),Fe=e.jsx("td",{children:e.jsx("span",{className:"cfih-api-type",children:"File"})}),s[85]=Oe,s[86]=Ee,s[87]=Ke,s[88]=Fe):(Oe=s[85],Ee=s[86],Ke=s[87],Fe=s[88]);let He;s[89]===Symbol.for("react.memo_cache_sentinel")?(He=e.jsx("div",{className:"cfih-table-wrap",style:ke,children:e.jsxs("table",{className:"cfih-table",children:[Ce,e.jsxs("tbody",{children:[Le,Ie,Te,Ae,Oe,Ee,e.jsxs("tr",{children:[Ke,Fe,e.jsxs("td",{children:["Optional ",e.jsx("code",{children:"File"})," object to load and analyze on mount"]})]}),e.jsxs("tr",{children:[e.jsx("td",{children:e.jsx("span",{className:"cfih-api-prop",children:"clearTrigger"})}),e.jsx("td",{children:e.jsx("span",{className:"cfih-api-type",children:"number"})}),e.jsx("td",{children:"Increment to programmatically clear the canvas"})]})]})]})}),s[89]=He):He=s[89];let Me;s[90]===Symbol.for("react.memo_cache_sentinel")?(Me=e.jsxs("section",{className:"cfih-section",children:[Se,we,He,e.jsx("div",{className:"cfih-callout",style:{marginTop:16},children:"A typical 16-color palette from a photographic image will include neutrals, mid-tones, and vivid accents, all perceptually well-separated in OKLab space."})]}),s[90]=Me):Me=s[90];let Ge;s[91]===Symbol.for("react.memo_cache_sentinel")?(Ge=e.jsxs("h2",{className:"cfih-section-title",children:[e.jsx(ss,{})," References"]}),s[91]=Ge):Ge=s[91];let ze;s[92]===Symbol.for("react.memo_cache_sentinel")?(ze=e.jsx("span",{className:"cfih-ref-num",children:"1"}),s[92]=ze):ze=s[92];let We,qe;s[93]===Symbol.for("react.memo_cache_sentinel")?(We=e.jsxs("li",{children:[ze,e.jsxs("span",{children:["Ottosson, B. (2020). ",e.jsx("em",{children:"A perceptual color space for image processing."})," ","oklab.org"]})]}),qe=e.jsx("span",{className:"cfih-ref-num",children:"2"}),s[93]=We,s[94]=qe):(We=s[93],qe=s[94]);let De,Pe;s[95]===Symbol.for("react.memo_cache_sentinel")?(De=e.jsxs("li",{children:[qe,e.jsxs("span",{children:["Arthur, D. & Vassilvitskii, S. (2007)."," ",e.jsx("em",{children:"K-Means++: The advantages of careful seeding."})," SODA '07."]})]}),Pe=e.jsx("span",{className:"cfih-ref-num",children:"3"}),s[95]=De,s[96]=Pe):(De=s[95],Pe=s[96]);let Ue,Ve;s[97]===Symbol.for("react.memo_cache_sentinel")?(Ue=e.jsxs("li",{children:[Pe,e.jsxs("span",{children:["Heckbert, P.S. (1982)."," ",e.jsx("em",{children:"Color image quantization for frame buffer display."})," SIGGRAPH '82."]})]}),Ve=e.jsx("span",{className:"cfih-ref-num",children:"4"}),s[97]=Ue,s[98]=Ve):(Ue=s[97],Ve=s[98]);let $e;return s[99]===Symbol.for("react.memo_cache_sentinel")?($e=e.jsxs("div",{className:"cfih-page",children:[t,o,e.jsxs("main",{className:"cfih-main",children:[p,je,Ne,Me,e.jsxs("section",{className:"cfih-section",children:[Ge,e.jsxs("ol",{className:"cfih-refs",children:[We,De,Ue,e.jsxs("li",{children:[Ve,e.jsxs("span",{children:["Lloyd, S.P. (1982)."," ",e.jsx("em",{children:"Least squares quantization in PCM."})," IEEE Transactions on Information Theory 28(2)."]})]})]})]})]}),e.jsx(Je,{})]}),s[99]=$e):$e=s[99],$e}export{is as default};
